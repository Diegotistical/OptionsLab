# src/pricing_models/monte_carlo_ml.py
"""
Machine Learning surrogate model for Monte Carlo option pricing.

This module provides a fast ML-based surrogate that learns to predict
option prices and Greeks from a Monte Carlo pricer. Once trained, predictions
are orders of magnitude faster than running full Monte Carlo simulations.

Features:
    - LightGBM for fast training and inference
    - Vectorized training data generation
    - Feature engineering (moneyness, normalized time, etc.)
    - Model persistence (save/load trained models)
    - Support for both calls and puts

Example:
    >>> from src.pricing_models.monte_carlo_ml import MonteCarloMLSurrogate
    >>> surrogate = MonteCarloMLSurrogate()
    >>> surrogate.fit(n_samples=5000)  # Train on generated data
    >>> predictions = surrogate.predict(test_df)
    >>> surrogate.save("models/saved_models/mc_surrogate.joblib")
"""

import logging
import os
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, Tuple, Union

import numpy as np
import pandas as pd

# Try to import LightGBM, fall back to sklearn if not available
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Try to import joblib for model persistence
try:
    import joblib
    JOBLIB_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False

from src.pricing_models.monte_carlo import MonteCarloPricer

__all__ = ["MonteCarloMLSurrogate", "LIGHTGBM_AVAILABLE"]

# Configure logger
logger = logging.getLogger(__name__)


class MonteCarloMLSurrogate:
    """
    Machine Learning surrogate for fast option pricing and Greeks.

    This class trains an ML model to approximate Monte Carlo option
    pricing without the computational cost of running simulations.
    The surrogate learns from training data generated by a Monte Carlo
    pricer and can then make instant predictions.

    Attributes:
        mc_pricer (MonteCarloPricer): The Monte Carlo pricer used for training.
        model: The trained ML model (LightGBM or sklearn fallback).
        trained (bool): Whether the model has been trained.
        feature_names (List[str]): Names of input features.
        target_names (List[str]): Names of prediction targets.

    Example:
        >>> surrogate = MonteCarloMLSurrogate(num_simulations=50000)
        >>> surrogate.fit(n_samples=3000)
        >>> # Fast prediction
        >>> df = pd.DataFrame({'S': [100], 'K': [100], 'T': [1.0],
        ...                    'r': [0.05], 'sigma': [0.2], 'q': [0.0]})
        >>> result = surrogate.predict(df)
        >>> print(f"Price: {result['price'].iloc[0]:.4f}")
    """

    # Constants for feature engineering
    FEATURE_COLUMNS = ["S", "K", "T", "r", "sigma", "q"]
    ENGINEERED_FEATURES = ["moneyness", "log_moneyness", "sqrt_T", "T_sigma"]
    TARGET_COLUMNS = ["price", "delta", "gamma"]

    def __init__(
        self,
        num_simulations: int = 50000,
        num_steps: int = 100,
        seed: Optional[int] = 42,
        use_numba: bool = True,
        model_type: Literal["lightgbm", "sklearn"] = "lightgbm",
        n_estimators: int = 200,
        max_depth: int = 6,
        learning_rate: float = 0.1,
        n_jobs: int = -1,
    ) -> None:
        """
        Initialize the ML surrogate.

        Args:
            num_simulations: Number of MC simulations for training data.
            num_steps: Number of time steps in MC simulation.
            seed: Random seed for reproducibility.
            use_numba: Whether to use Numba acceleration in MC pricer.
            model_type: ML model type - 'lightgbm' (fast) or 'sklearn' (fallback).
            n_estimators: Number of boosting rounds / trees.
            max_depth: Maximum tree depth.
            learning_rate: Learning rate for gradient boosting.
            n_jobs: Number of parallel jobs (-1 for all cores).
        """
        self.num_simulations = num_simulations
        self.num_steps = num_steps
        self.seed = seed
        self.use_numba = use_numba
        self.n_jobs = n_jobs
        
        # Initialize MC pricer for training data generation
        self.mc_pricer = MonteCarloPricer(
            num_simulations=num_simulations,
            num_steps=num_steps,
            seed=seed,
            use_numba=use_numba,
        )
        
        # Build ML model
        self.model_type = model_type if LIGHTGBM_AVAILABLE else "sklearn"
        self.model = self._build_model(n_estimators, max_depth, learning_rate)
        self.trained = False
        
        # Metadata
        self.feature_names = self.FEATURE_COLUMNS + self.ENGINEERED_FEATURES
        self.target_names = self.TARGET_COLUMNS
        self._training_stats: Dict[str, Any] = {}
        
        logger.info(
            f"Initialized MonteCarloMLSurrogate with model_type='{self.model_type}', "
            f"n_estimators={n_estimators}, max_depth={max_depth}"
        )

    def _build_model(
        self,
        n_estimators: int,
        max_depth: int,
        learning_rate: float,
    ) -> Any:
        """
        Build the ML model pipeline.

        Args:
            n_estimators: Number of boosting iterations.
            max_depth: Maximum depth of trees.
            learning_rate: Boosting learning rate.

        Returns:
            Configured ML model pipeline.
        """
        if self.model_type == "lightgbm" and LIGHTGBM_AVAILABLE:
            base_model = lgb.LGBMRegressor(
                n_estimators=n_estimators,
                max_depth=max_depth,
                learning_rate=learning_rate,
                random_state=self.seed,
                n_jobs=self.n_jobs,
                verbose=-1,  # Suppress output
                force_col_wise=True,  # Better for small datasets
            )
        else:
            base_model = GradientBoostingRegressor(
                n_estimators=n_estimators,
                max_depth=max_depth,
                learning_rate=learning_rate,
                random_state=self.seed,
            )
        
        # Wrap in MultiOutputRegressor for multiple targets
        return Pipeline([
            ("scaler", StandardScaler()),
            ("regressor", MultiOutputRegressor(base_model, n_jobs=self.n_jobs)),
        ])

    def _engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Add engineered features to improve model accuracy.

        Features added:
            - moneyness: S / K (spot to strike ratio)
            - log_moneyness: log(S / K)
            - sqrt_T: sqrt(time to maturity)
            - T_sigma: T * sigma (volatility-time product)

        Args:
            df: DataFrame with base features (S, K, T, r, sigma, q).

        Returns:
            DataFrame with additional engineered features.
        """
        result = df.copy()
        
        # Moneyness features
        result["moneyness"] = result["S"] / result["K"]
        result["log_moneyness"] = np.log(result["moneyness"])
        
        # Time-related features
        result["sqrt_T"] = np.sqrt(result["T"])
        result["T_sigma"] = result["T"] * result["sigma"]
        
        return result

    def _generate_random_params(
        self,
        n_samples: int,
        S_range: Tuple[float, float] = (50.0, 200.0),
        K_range: Tuple[float, float] = (50.0, 200.0),
        T_range: Tuple[float, float] = (0.05, 2.0),
        r_range: Tuple[float, float] = (0.01, 0.10),
        sigma_range: Tuple[float, float] = (0.05, 0.60),
        q_range: Tuple[float, float] = (0.0, 0.05),
    ) -> pd.DataFrame:
        """
        Generate random option parameters for training.

        Uses stratified sampling to ensure good coverage across
        the parameter space.

        Args:
            n_samples: Number of samples to generate.
            S_range: (min, max) for spot price.
            K_range: (min, max) for strike price.
            T_range: (min, max) for time to maturity.
            r_range: (min, max) for risk-free rate.
            sigma_range: (min, max) for volatility.
            q_range: (min, max) for dividend yield.

        Returns:
            DataFrame with n_samples rows of random parameters.
        """
        rng = np.random.default_rng(self.seed)
        
        # Generate parameters with uniform distribution
        data = {
            "S": rng.uniform(S_range[0], S_range[1], n_samples),
            "K": rng.uniform(K_range[0], K_range[1], n_samples),
            "T": rng.uniform(T_range[0], T_range[1], n_samples),
            "r": rng.uniform(r_range[0], r_range[1], n_samples),
            "sigma": rng.uniform(sigma_range[0], sigma_range[1], n_samples),
            "q": rng.uniform(q_range[0], q_range[1], n_samples),
        }
        
        return pd.DataFrame(data)

    def generate_training_data(
        self,
        n_samples: int = 5000,
        option_type: Literal["call", "put"] = "call",
        param_ranges: Optional[Dict[str, Tuple[float, float]]] = None,
        verbose: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate training data using Monte Carlo simulation.

        This method creates training examples by running the MC pricer
        on randomly generated option parameters.

        Args:
            n_samples: Number of training samples to generate.
            option_type: Option type for training ('call' or 'put').
            param_ranges: Optional dict with custom parameter ranges.
            verbose: Whether to print progress updates.

        Returns:
            Tuple of (X, y) where X is features and y is targets.

        Example:
            >>> surrogate = MonteCarloMLSurrogate()
            >>> X, y = surrogate.generate_training_data(n_samples=1000)
            >>> print(f"Generated {X.shape[0]} samples with {X.shape[1]} features")
        """
        if verbose:
            logger.info(f"Generating {n_samples} training samples...")
        
        # Generate random parameters
        if param_ranges:
            df = self._generate_random_params(n_samples, **param_ranges)
        else:
            df = self._generate_random_params(n_samples)
        
        # Pre-allocate target arrays
        prices = np.zeros(n_samples)
        deltas = np.zeros(n_samples)
        gammas = np.zeros(n_samples)
        
        # Compute MC prices and Greeks (vectorized where possible)
        for i in range(n_samples):
            row = df.iloc[i]
            S, K, T, r, sigma, q = (
                row["S"], row["K"], row["T"], row["r"], row["sigma"], row["q"]
            )
            
            try:
                # Use consistent seed for reproducibility
                seed = self.seed + i if self.seed else None
                
                prices[i] = self.mc_pricer.price(
                    S, K, T, r, sigma, option_type, q, seed=seed
                )
                delta, gamma = self.mc_pricer.delta_gamma(
                    S, K, T, r, sigma, option_type, q, seed=seed
                )
                deltas[i] = delta
                gammas[i] = gamma
                
            except Exception as e:
                logger.warning(f"Failed to compute sample {i}: {e}")
                prices[i] = 0.0
                deltas[i] = 0.5 if option_type == "call" else -0.5
                gammas[i] = 0.01
            
            if verbose and (i + 1) % 500 == 0:
                logger.info(f"Generated {i + 1}/{n_samples} samples")
        
        # Engineer features
        df_features = self._engineer_features(df)
        X = df_features[self.feature_names].values
        y = np.column_stack([prices, deltas, gammas])
        
        if verbose:
            logger.info(f"Training data generation complete: X{X.shape}, y{y.shape}")
        
        return X, y

    def fit(
        self,
        X: Optional[Union[pd.DataFrame, np.ndarray]] = None,
        y: Optional[np.ndarray] = None,
        n_samples: int = 5000,
        option_type: Literal["call", "put"] = "call",
        verbose: bool = True,
    ) -> "MonteCarloMLSurrogate":
        """
        Train the ML surrogate model.

        If X and y are not provided, training data is automatically
        generated using Monte Carlo simulation.

        Args:
            X: Feature matrix (optional). If DataFrame, must have columns
               S, K, T, r, sigma, q.
            y: Target matrix (optional). Shape (n_samples, 3) for
               [price, delta, gamma].
            n_samples: Number of samples to generate if X/y not provided.
            option_type: Option type for auto-generated data.
            verbose: Whether to print progress updates.

        Returns:
            Self for method chaining.

        Example:
            >>> surrogate = MonteCarloMLSurrogate()
            >>> surrogate.fit(n_samples=5000)  # Auto-generate and train
            >>> # Or train on custom data:
            >>> surrogate.fit(X=custom_features, y=custom_targets)
        """
        if verbose:
            logger.info("Starting ML surrogate training...")
        
        # Generate training data if not provided
        if X is None or y is None:
            X, y = self.generate_training_data(
                n_samples=n_samples,
                option_type=option_type,
                verbose=verbose,
            )
        elif isinstance(X, pd.DataFrame):
            # Engineer features if DataFrame provided
            X_df = self._engineer_features(X)
            X = X_df[self.feature_names].values
        
        # Train the model
        if verbose:
            logger.info(f"Training {self.model_type} model on {X.shape[0]} samples...")
        
        self.model.fit(X, y)
        self.trained = True
        
        # Store training statistics
        self._training_stats = {
            "n_samples": X.shape[0],
            "n_features": X.shape[1],
            "n_targets": y.shape[1],
            "option_type": option_type,
        }
        
        if verbose:
            logger.info("Training complete!")
        
        return self

    def predict(
        self,
        X: Union[pd.DataFrame, np.ndarray],
    ) -> pd.DataFrame:
        """
        Predict option prices and Greeks using the trained surrogate.

        Args:
            X: Feature data. If DataFrame, must contain columns
               S, K, T, r, sigma, q. If ndarray, must match training features.

        Returns:
            DataFrame with columns 'price', 'delta', 'gamma'.

        Raises:
            RuntimeError: If model has not been trained.

        Example:
            >>> surrogate = MonteCarloMLSurrogate()
            >>> surrogate.fit()
            >>> df = pd.DataFrame({'S': [100, 110], 'K': [100, 100],
            ...                    'T': [1.0, 1.0], 'r': [0.05, 0.05],
            ...                    'sigma': [0.2, 0.2], 'q': [0.0, 0.0]})
            >>> predictions = surrogate.predict(df)
        """
        if not self.trained:
            raise RuntimeError(
                "Model has not been trained. Call fit() first."
            )
        
        # Handle DataFrame input
        if isinstance(X, pd.DataFrame):
            X_df = self._engineer_features(X)
            X_array = X_df[self.feature_names].values
        else:
            X_array = X
        
        # Make predictions
        predictions = self.model.predict(X_array)
        
        return pd.DataFrame(
            predictions,
            columns=self.target_names,
        )

    def predict_single(
        self,
        S: float,
        K: float,
        T: float,
        r: float,
        sigma: float,
        q: float = 0.0,
    ) -> Dict[str, float]:
        """
        Predict for a single option.

        Convenience method for single-option predictions.

        Args:
            S: Spot price.
            K: Strike price.
            T: Time to maturity.
            r: Risk-free rate.
            sigma: Volatility.
            q: Dividend yield.

        Returns:
            Dictionary with 'price', 'delta', 'gamma'.
        """
        df = pd.DataFrame({
            "S": [S],
            "K": [K],
            "T": [T],
            "r": [r],
            "sigma": [sigma],
            "q": [q],
        })
        
        result = self.predict(df)
        return {
            "price": float(result["price"].iloc[0]),
            "delta": float(result["delta"].iloc[0]),
            "gamma": float(result["gamma"].iloc[0]),
        }

    def save(self, path: Union[str, Path]) -> None:
        """
        Save the trained model to disk.

        Args:
            path: File path for the saved model (should end in .joblib).

        Raises:
            RuntimeError: If model has not been trained.
            ImportError: If joblib is not installed.

        Example:
            >>> surrogate.fit()
            >>> surrogate.save("models/saved_models/mc_surrogate.joblib")
        """
        if not self.trained:
            raise RuntimeError("Cannot save untrained model. Call fit() first.")
        
        if not JOBLIB_AVAILABLE:
            raise ImportError(
                "joblib is required for model persistence. "
                "Install with: pip install joblib"
            )
        
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        save_dict = {
            "model": self.model,
            "model_type": self.model_type,
            "feature_names": self.feature_names,
            "target_names": self.target_names,
            "training_stats": self._training_stats,
            "seed": self.seed,
        }
        
        joblib.dump(save_dict, path)
        logger.info(f"Model saved to {path}")

    @classmethod
    def load(cls, path: Union[str, Path]) -> "MonteCarloMLSurrogate":
        """
        Load a trained model from disk.

        Args:
            path: Path to the saved model file.

        Returns:
            Loaded MonteCarloMLSurrogate instance.

        Example:
            >>> surrogate = MonteCarloMLSurrogate.load(
            ...     "models/saved_models/mc_surrogate.joblib"
            ... )
            >>> predictions = surrogate.predict(test_data)
        """
        if not JOBLIB_AVAILABLE:
            raise ImportError(
                "joblib is required for model persistence. "
                "Install with: pip install joblib"
            )
        
        save_dict = joblib.load(path)
        
        # Create instance without expensive initialization
        instance = cls.__new__(cls)
        instance.model = save_dict["model"]
        instance.model_type = save_dict["model_type"]
        instance.feature_names = save_dict["feature_names"]
        instance.target_names = save_dict["target_names"]
        instance._training_stats = save_dict["training_stats"]
        instance.seed = save_dict["seed"]
        instance.trained = True
        instance.mc_pricer = None  # Not needed for inference
        
        logger.info(f"Model loaded from {path}")
        return instance

    def score(
        self,
        X: Union[pd.DataFrame, np.ndarray],
        y_true: np.ndarray,
    ) -> Dict[str, float]:
        """
        Evaluate model performance on test data.

        Computes R² score and RMSE for each target.

        Args:
            X: Feature data.
            y_true: True target values.

        Returns:
            Dictionary with R² and RMSE for each target.
        """
        y_pred = self.predict(X).values
        
        scores = {}
        for i, name in enumerate(self.target_names):
            ss_res = np.sum((y_true[:, i] - y_pred[:, i]) ** 2)
            ss_tot = np.sum((y_true[:, i] - np.mean(y_true[:, i])) ** 2)
            r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
            rmse = np.sqrt(np.mean((y_true[:, i] - y_pred[:, i]) ** 2))
            
            scores[f"{name}_r2"] = r2
            scores[f"{name}_rmse"] = rmse
        
        return scores


# Backward compatibility alias
MonteCarloML = MonteCarloMLSurrogate
